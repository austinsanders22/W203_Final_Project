---
title: "Keeping it PG-13"
author: "Steve Hewitt, Martin Lim, Fidelia Nawar, & Austin Sanders"
date: "12/9/2021"
output: 
  pdf_document: 
    fig_caption: true
    number_sections: true
---

\tableofcontents
\newpage

```{r - Setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

MIN_REVENUE = 1
MIN_BUDGET = 1

library(gridExtra)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(hash)
library(lmtest)
library(sandwich)
library(stargazer)
library(hash)
library(caTools)
library(GGally)
library(ggpubr)
library(kableExtra)

#data <- read.csv(file = "../data/movies.csv")
#cpi <- read.csv(file = "../data/CPI_by_Year.csv")

data <- read.csv(file = "movies.csv")
cpi <- read.csv(file = "CPI_by_Year.csv")
```

# Introduction

## Context

> Acme Studios has spent a $50,000,000.00 budget on a superhero movie, and the director insists that the film should include a scene where the main villain goes on an expletive-laden tirade. We know that having this scene will mean that the movie will be rated R, and cutting the scene will result in the movie being rated PG-13. The director is extremely upset that we want to cut the scene and says we're ruining the film's artistic integrity by making editorial changes after the director's cut. He's so upset that he went directly to the studio head to complain. Now Acme Studios' executive team has to decide: do they modify the movie for a more family-friendly rating, or do they respect the director's wishes and release it as-is? As data scientists, we would have difficulty quantifying artistic integrity or the value of the relationship between the studio and the director. Still, we feel strongly that we can show the relationship between worldwide revenue for a PG-13 vs. R ratings (holding all other variables constant). The studio head wants to know: How much more money do they expect to make by defying the director's wishes and cutting the movie to make it PG-13?

## Research Question

>   *Holding other factors constant, how much more money should a movie studio expect to make on a film that gets a PG-13 rating instead of an R rating from the MPAA?*

>   Our research question intends to measure the impact of *MPAA Rating* and *Movie Quality* on the *Gross Revenue* that it will generate. Given other available, quantifiable factors like the *Budget*, *Genre*, and *Runtime*, this study also intends to investigate if they affect the *MPAA Rating* and/or *Movie Quality* which in turn affect the *Gross Revenue*. 

## Causal Theory

![](causal_theory.png){width=60%}


>   Our research question seeks to measure the impact of the *MPAA Rating* (more specifically, PG-13 versus R) on the *Gross box office revenue*. A movie typically receives an R rating by the MPAA for some combination of violent content, adult content, explicit language, and drug use. In addition to these factors contributing to the MPAA rating, they also contribute to the quality of the movie. We expect to show that both MPAA rating and movie quality impact the film's gross revenue. By adjusting the movie's content to secure the desired rating, we may also affect the quality of the movie. Therefore, this study will explore models that include a proxy for movie quality to attempt to minimize omitted variable bias.

# Research Design and Data

## Data Source

>   The data used for this study is a movie dataset from Kaggle (https://www.kaggle.com/danielgrijalvas/movies). It contains 7512 unique movie titles ranging from the year 1980 to 2020. According to the description of the creator of the dataset, the data was scraped from IMDb.com so the extent of the movie title coverage can go as far as the available information posted on the IMDb website. Below are the important columns that were considered for this study:

* Outcome Variable
    + `gross`   : Revenue of the movie in USD
\
* Explanatory Variables
    + `score`   : Average IMDb user rating
    + `rating`  : MPAA rating of the movie (R, PG, etc.)
    + `runtime` : Length of movie in minutes
    + `budget`  : Budget of the movie in USD
    + `votes`   : Number of user votes on the IMDb website
    
## Research Design

>   Using the data and variables above, this study measured the impact *MPAA Rating* and *Movie Quality* on the *Gross Revenue* generated by a movie. Causal models were generated using the logic from the causal theory in section 1.3. `gross` is the primary quantified success variable. According to the causal theory, two main explanatory quantities were included in the model. `rating` was used to quantify the MPAA rating of the film while `score` is the main quantified measurement of quality. `budget` and `runtime` both affect the movie quality, but not the `rating` so they were considered as proxies for quality.

>   The outcome of interest is `gross`, the movie's revenue in USD. This is the main outcome variable of all the causal models and is quantified in numeric form.

> To properly quantify `rating` in the model, indicator variables were used. The data was divided into two (2) categories: PG-13 and R. All movies rated PG-13 were given a separate indicator variable (`PG13`) while all R movies were treated as the base case.

>   The `score` variable primarily measures movie quality. Variables like `budget` and `runtime` also affect quality, so they were considered as possible proxies for quality in the causal models.

>   In order to remove duplicated movie titles, the data point with the larger budget was retained. If both the budgets and titles were equal, the data point with the larger revenue was retained.

>   Because our dataset covers 50 years, we apply a CPI-based price adjustment to each monetary variable to account for inflation. The inflation-adjusted values (in 2020 dollars) will be represented with the 'adj_gross' and 'adj_budget' variables.

>   The filtered dataset was used to produce multiple linear models and evaluate them using coefficient tests in R. Stargazer was used to compare the models to determine which model best aligns with the data and our causal theory. Armed with this chosen model, its predictions were evaluated to check how well the model can predict the revenue based on the input parameters. Finally, the model was applied to the specific case outlined in the overview section above to predict the revenue for both a PG-13 and R MPAA rating case.

## Data Cleaning
>   Removed entries with budget under `r MIN_BUDGET` or gross revenue under `r MIN_REVENUE` to filter out small-scale releases that do not fit the mold of the type of movie we want to measure.

>   Removed entries that were not rated PG-13 or R.

>   Removed duplicate entries as discussed in the previous section.

>   Created indicator variable for PG-13 rating.

>   Created CPI-adjusted variables for gross revenue and budget.


```{r Data Cleaning}
# Retrieve only the needed columns
df_raw = select(data, c('title', 'gross', 'budget', 'score', 'runtime', 'rating', 'year', 'votes'))

# Remove data points with world_revenue under MIN_REVENUE
df_raw <- subset(df_raw, df_raw$gross >= MIN_REVENUE & !is.na(df_raw$gross))

# Remove data points with budget under MIN_BUDGET
df_raw <- subset(df_raw, df_raw$budget >= MIN_BUDGET & !is.na(df_raw$budget))

# Keep PG-13 and R data points
df_raw <- subset(df_raw, df_raw$rating == "PG-13" | df_raw$rating == "R")

# Remove Duplicate Movie Titles
# Hash object for title : budget
h <- hash() 

# Clean dataframe
df = data.frame()

for(i in 1:nrow(df_raw)) {       # for-loop over rows
  title_key = df_raw[i,'title']
  
  if (TRUE == all(has.key( title_key, h ))) {
    # Title is already recorded

    # Search for existing row in clean dataframe with the same title
    for (k in 1:nrow(df)) {
      
      if (title_key == df[k,'title']) {
        # Replace row if the budget of the new value is higher than that of the
        # budget of the recorded title
        if (df_raw[i, 'budget'] > df[k, 'budget']) {
          
          # Delete found row in cleaned dataframe
          df = df[-c(k),]
          
          # Bind raw dataframe row to clean dataframe
          df <- rbind(df, df_raw[i,])
          
          # Revise title_key and budget to hash
          h[[title_key]] = df_raw[i,'budget']
        } else if (df_raw[i, 'budget'] > df[k, 'budget']
                   & df_raw[i, 'gross'] > df[k, 'gross']) {
          # Delete found row in cleaned dataframe
          df = df[-c(k),]
          
          # Bind raw dataframe row to clean dataframe
          df <- rbind(df, df_raw[i,])
          
          # Revise title_key and budget to hash
          h[[title_key]] = df_raw[i,'budget']
        }
        break
      }
      
    }
    
  } else {
    # Add title_key and budget to hash
    h[[title_key]] = df_raw[i,'budget']
    
    # Bind raw dataframe row to clean dataframe
    df <- rbind(df, df_raw[i,])
  }
}

# Apply Indicator variable for PG-13 (R is the base case)
df$PG13 = factor(ifelse(df$rating == "PG-13" , 1, 0))

# Perform CPI adjustment to both gross and budget variables.
df <- merge(x = df, y = cpi, by= 'year')
df <- transform(df, adj_gross = gross * adjust_to_2020_dollars, adj_budget = budget * adjust_to_2020_dollars)
```

## Exploratory Data Analysis

>   There are `r nrow(df)` unique titles considered for this study.

```{r - EDA: Check main financial variables}
gross_histogram <- df %>%
  ggplot(aes(adj_gross)) +
  geom_histogram(color = "black", fill = "light blue", bins=30)

log_gross_histogram <- df %>%
  ggplot(aes(log(adj_gross))) +
  geom_histogram(color = "black", fill = "light blue", bins=30)

budget_histogram <- df %>%
  ggplot(aes(adj_budget)) +
  geom_histogram(color = "black", fill = "red", bins=30)

log_budget_histogram <- df %>%
  ggplot(aes(log(adj_budget))) +
  geom_histogram(color = "black", fill = "red",bins=30)

grid.arrange(gross_histogram, log_gross_histogram,
             budget_histogram, log_budget_histogram,
             nrow = 2, ncol = 2, top = "Financial Variable distributions")

```

>   Looking at the adj_budget vs. count and adj_gross vs. count graphs, we can see how there is a significant skew towards the left. Based on our EDA, log transformations of both the gross revenue (`gross`) and `budget` variables seemed helpful to produce a better-fitting linear model. This is because of the skewed nature of the data where both the financial variables have most data points toward zero with some extreme outliers (movies that generated a very large revenue). 

```{r - EDA: Check other explanatory variables, warning=FALSE}
# CHECK OTHER EXPLANATORY VARIABLES

df$ratings_distribution = factor(ifelse(df$rating == "R", "R", ifelse(df$rating == "PG-13", "PG-13", "G/PG")))

ratings_distribution_histogram <- df %>%
  ggplot(aes(x=ratings_distribution)) +
  geom_bar(color = "blue", fill = rgb(.1,.4, .5,.7)) +
  theme(legend.position = "none")

score_histogram <- df %>%
  ggplot(aes(score)) +
  geom_histogram(color = "black", fill = "blue",bins=30)

year_histogram <- df %>%
  ggplot(aes(year)) +
  geom_histogram(color = "black", fill = "orange",bins=10)

votes_histogram <- df %>%
  ggplot(aes(votes)) +
  geom_histogram(color = "black", fill = "green",bins=10)

log_votes_histogram <- df %>%
  ggplot(aes(log(votes))) +
  geom_histogram(color = "black", fill = "green",bins=30)

runtime_histogram <- df %>%
  ggplot(aes(runtime)) +
  geom_histogram(color = "white", fill = "black",bins=30)

grid.arrange(ratings_distribution_histogram, score_histogram,  
             votes_histogram, log_votes_histogram,
             runtime_histogram, year_histogram,
             nrow = 3, ncol = 2, top = "Explanatory Variable Distributions")
```

>   Looking at the ratings_distribution graph after EDA, we can see how the dataset contains at least 1,000 more rated R movies than PG-13 movies. This could potentially affect our training/testing sets and results because we have more R data movies to work with rather than PG-13 but should still be appropriate to answer our research question. For the second graph of score vs. count, we see a relatively normal distribution that is slightly off-centered to the right with a peak around 6.25. This bell curve is expected because the ratings are scored from 0-10 and it is expected that the peak will be at the average rating (at around 5). The votes vs. count graph has a left skew with a heavy tail near 0. This is due to most of the movies in the dataset having no more than a couple thousand or tens of thousands of reviews. This plot is not as visually intuitive, so by taking the log(votes), we can see a much cleaner normal distrbution with an off-center to the right once again. The runtime vs. count graph also has a normal distribution but centered to the left at about 100 minutes, meaning most of the movies in the dataset don't run longer than an hour and a half to two hours. For the final graph, we see a relatively even distribution of the number of movies released each year, except during 1980-1990 where fewer movies were released in the dataset than in 2000 or 2010. Overall, the visualizations of the histograms from the data seem to follow our expectations. The other numeric variables (except for gross revenue (`gross`) and `budget`) did not seem to require any transformations because the distribution seemed symmetrical enough.

```{r - Evaluate numeric variables for perfect collinearity, message = FALSE, warning = FALSE, error = FALSE}

df_collinearity = select(df, c('adj_gross', 'adj_budget', 'score', 'runtime', 'year', 'votes'))

# Log-Transform `adj_gross`, `adj_budget`, and `votes`
df_collinearity$log_adj_gross = log(df_collinearity$adj_gross)
df_collinearity$log_adj_budget = log(df_collinearity$adj_budget)
df_collinearity$log_votes = log(df_collinearity$votes)

# Remove `gross`, `budget`, and `votes`
df_collinearity = select(df_collinearity, c('log_adj_gross', 'log_adj_budget', 'score', 'runtime', 'year', 'log_votes'))

# check for collinearity
ggpairs(df_collinearity) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

>   According to the correlation plots above, the explanatory variables do not seem to have any strong correlation to each other. The log-transformed data for `adj_budget` and `votes` may have decent correlation with the log-transformed data for `adj_gross` (the outcome variable), but this should not produce any problems for finding an BLP for this data.


# Statistical Model - rearranged, move some parts to results

## A Model Building Process

```{r - Create the models}

#Model 1: Rating Only
# Base Case - R Movies
model_1 = lm(log(adj_gross) ~ PG13, data = df)

#Model 2: Rating and Quality
# Base Case - R Movies with rating of 0
model_2 = lm(log(adj_gross) ~ PG13 + score, data = df)

#Model 3: Rating and Quality with interaction terms
model_3 = lm(log(adj_gross) ~ PG13 + score + PG13*score, data = df)

#Model 4: Rating and Quality with budget
model_4 = lm(log(adj_gross) ~ PG13 + score + log(adj_budget), data = df)

#Model 5: Rating and Quality with budget and interaction terms
model_5 = lm(log(adj_gross) ~ PG13 + score + PG13 * score + log(adj_budget) + log(adj_budget) * PG13, data = df)

```

```{r - Function to interpret coefficients}
level_to_log <- function(coeff) {
  # Inputs the coefficient of an untransformed variable
  # Returns the % change in the log-transformed outcome variable per unit increase in coeff
  return ((exp(coeff)-1)*100)
}

log_log_pct_change <- function(coeff, pct_change) {
  # Inputs the coefficient of an untransformed variable and the percent change
  # Returns the % change in the log-transformed outcome variable given the % increase in the log-transformed explanatory variable
  # Basis: https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9
  input_ratio = 1+(pct_change/100)
  output_ratio = input_ratio ^ coeff
  return ((output_ratio-1)*100)
}

```

> Our primary input variable of interest is the *MPAA rating*, with an *R* rating as the base case and a *PG-13* rating as the alternative case. *MPAA rating* is a categorical variable with most of the dataset falling under the *R* rating and a sizable minority falling under the *PG-13* rating.

> Our causal theory has identified *Movie Quality* as another key input variable to measure because there is a relationship between the factors that determine *MPAA rating* and *Movie Quality*, which also influence *gross revenue*. Because we don't have an exact measurement of *Movie Quality*, we use the IMDb Rating (*score*) as a proxy to help mitigate omitted variable bias. The distribution of this variable seems roughly normally distributed, so no transformations were applied.

> Lastly, we are also prepared to consider a model that includes *budget* as a parameter because it has a strong correlation to *gross revenue*, and including it has the potential to make the predictions from our model much more accurate even if it is not directly named in our causal theory. Including *budget* has the potential to absorb some of the effect we are measuring, but there is also the potential of value in examining the interaction between *budget* and *MPAA rating*; we may find that a certain rating provides better return as the budget increases, or conversely that a certain rating does well with even very small budgets. This variable has a lopsided distribution with a heavy tail near zero and quickly becomes sparse moving to the right. Applying a log transformation makes this variable appear more evenly distributed.

> We took an iterative approach to model building by starting with something very simple and adding terms as we went along until we reached something that satisfied our need for something that was both significant and sufficiently predictive.

\


## Proposed Models:

\
(1) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 $$

> We started with the most simplistic possible model to see if a measurable difference existed between the *gross revenue* of an *R* or *PG-13* rated movie. This first model (1) measures only the effect of changing the MPAA rating (PG-13 or R) on the *gross revenue* of the movie. $\beta_0$ is the *gross revenue* that an *R*-rated movie is estimated to generate, while $\beta_1$ is the estimated gain in *gross revenue* if the movie is instead rated *PG-13*. This model is too simple to be our final choice, but if it had shown no significant difference in *gross revenue* it would have been a major red flag.


\
(2) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 + \beta_2 * Score $$

> The second model (2) introduced the effect of a film's *IMDb score* on the *gross revenue* on top of the components of model (1). $\beta_2$ indicates the estimated percentage increase in *gross revenue* per one-point increase in *IMDb score*. Including *score* was considered a bare minimum requirement for our final model because we are using it as a proxy for *Movie Quality* from our causal theory.


\
(3) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 + \beta_2 * Score + \beta_3 * PG13 * Score $$

> The third model (3) introduced an interaction term between a film's *IMDb score* and *rating*, alongside the components of model (2). $\beta_3$ indicates the estimated percentage increase in revenue per one-point increase in *IMDb score* if the movie is rated *PG-13*. Like in the previous model $\beta_2$ indicates the estimated percentage increase in *gross revenue* per one-point increase in *IMDb score*, but having an interaction term allows *score* to impact predictions for *PG-13* and *R* differently. The magnitude of $\beta_3$ shows if the change in *gross revenue* per point of *score* is different for a *PG-13* movie than for an *R* movie.


\
(4) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 + \beta_2 * Score + \beta_3 * ln(Budget)$$

>  The fourth model (4) was built on top of model (2), with the addition of a term for *budget*. Unlike model (3), an interaction term between *MPAA rating* and *score* was not included. The purpose of this model was to offer something with more predictive power than the previous models by including *budget*. We know that *gross revenue* and *budget* are strongly correlated, and we expected to see this model perform better at predicting results (as measured by adjusted R-squared) than the previous models. The term $\beta_3$ in this model represents the relationship between percentage increases in *budget* and percentage increases in *gross revenue*; for every 1% increase in *budget* we expect a $\beta_3$% increase in *gross revenue*.


\
(5) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 + \beta_2 * Score + \beta_3 * PG13 * Score$$
$$+ \beta_4 * ln(Budget) + \beta_5 * PG13 * ln(Budget) $$

> The fifth and final model (5) builds on all of the models before it. The terms $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ have identical meanings to model (3). $\beta_4$ now represents the relationship between percentage increases in *budget* and percentage increases in *gross revenue*. The new term $\beta_5$ is an interaction term between *MPAA rating* and *budget*. Having an interaction term allows *budget* to impact predictions for *PG-13* and *R* differently. The magnitude of $\beta_5$ shows if the percentage change in *gross revenue* per percentage increase in *budget* is different for a *PG-13* movie than it is for an *R* movie.


# Results

## Model Comparison

```{r create stargazer, results = "asis"}
stargazer(
  model_1, 
  model_2,
  model_3,
  model_4,
  model_5,
    covariate.labels = 
        c("PG13", "IMDb Score", "PG13 x Score", "PG13 x Log(Budget)",
          "Log(Budget)", "(Intercept)"),
  type = 'latex',
  header = FALSE, 
  omit.stat=c("f", "ser"),
  star.cutoffs = c(0.05, 0.01, 0.001) # the default isn't in line with w203
)
```

> Of all models considered, our team determined that model (5) was the most effective. It produces the best-quality predictions of all models, as shown by the adjusted R-squared score. It also captures the all of the effects that any of the models flagged as being significant. The one term that is not significant in this model is the interaction term between PG-13 and score, with a standard error that overlaps zero. Model (2) adhered most strictly to the causal theory we hoped to advance, but the vastly superior performance of model (5) could not be ignored. We performed an F-test on these models and it reinforced our decision to reject the null (simple) model (2) in favor of the fuller model (5).

> Based on the findings of the results table above, we can see how our primary variable of interest, rating of PG-13, is expected to generate less revenue than movies rated R for every 1 point increase in the score (which is our proxy for movie quality). It's also interesting to note that even if there is an increase in the rating of a PG13 movie, it is still expected to generate less revenue than a rated-R movie with a similar rise in rating. In general, we see how an increase in a film's score drastically increases how much gross revenue a movie generates (52.02%). There is also a positive relationship between increasing budget to generate more revenue but only by 7.69%. Therefore, we can conclude that improving the quality of the movie (score) increases the gross revenue generated, as expected. Compared to our original prediction, though, PG-13 movies are not gaining more revenue in every case (only when the budget is high).

```{r F-test}
anova(model_2, model_5, test='F')
```

## Model Interpretation

```{r extra betas from model}
m5b0 = round(summary(model_5)$coefficients[1], digits = 3)
m5b1 = round(summary(model_5)$coefficients[2], digits = 3)
m5b2 = round(summary(model_5)$coefficients[3], digits = 3)
m5b3 = round(summary(model_5)$coefficients[5], digits = 3)
m5b4 = round(summary(model_5)$coefficients[4], digits = 3)
m5b5 = round(summary(model_5)$coefficients[6], digits = 3)
m5b0_i = round(exp(summary(model_5)$coefficients[1]), digits = 3)
m5b1_i = round(exp(m5b0+m5b1), digits = 3)
m5b2_i = round(level_to_log(m5b2), digits = 3)
m5b3_i = round(level_to_log(m5b3), digits = 3)
m5b4_i = round(log_log_pct_change(m5b4,10), digits = 3)
m5b5_i = round(log_log_pct_change((m5b5+m5b4),10), digits = 3)
```

After running our linear regressions and calculating the beta coefficients, our final model (5) is set:
$$ ln(Gross) = `r m5b0` `r m5b1` * PG13 + `r m5b2` * Score `r m5b3` * PG13 * Score$$
$$ + `r m5b4` * ln(Budget) + `r m5b5` * PG13 * ln(Budget) $$

A closer look at each of the model coefficients and how they can be interpreted:


> $\beta_0 = `r m5b0`$ : As a baseline, movies rated *R* with a *budget* of 0 and an *IMDb score* of 0 are expected to generate `r format(m5b0_i, nsmall = 2)` in gross revenue.

> $\beta_1  = `r m5b1`$ : The baseline for *PG-13* movies with a *budget* of 0 and an *IMDb score* of 0 is lower than movies with an *R* rating. This puts the baseline prediction for *gross revenue* for a PG-13 movie at `r format(m5b1_i, nsmall = 2)` Although both $\beta_0$ and $beta_1$ are statistically significant, they are of little practical significance. We are not interested in movies with a *budget* of 0 or an *IMDb score* of 0. The other coefficients tell a more important story.

> $\beta_2  = `r m5b2`$ : For every one point increase in *IMDb Score* our *gross revenue* prediction increases by `r 100 * m5b2`%. This is both statistically and practically significant. It supports our prior assumption that *movie quality* as measured by *IMDb score* is an important factor in the success of a movie as measured by *gross revenue*.

> $\beta_3  = `r m5b3`$ : This is the first interaction term in our model, and it serves to illustrate the difference in impact that increasing *IMDb score* has on *PG-13* movies versus the *R* movie base case. For a PG-13 movie we combine the effects of $\beta_2$ and $\beta_3$ so for each one point increase in *IMDb* rating we expect to see a `r (m5b2 + m5b3) * 100 `% increase in *gross revenue*. This is the only coefficient in our model that is not statistically significant, so it is not particularly valuable in drawing practical conclusions. It suggests that *PG-13* movies benefit less per point increase in *IMDb score*, but the relationship is small and the standard error overlaps zero.

> $\beta_4 = `r m5b4`$ : Because *budget* proved to be a strong predictor of *gross revenue*, this term is especially important. Due to both variables having undergone a log transformation, the interpretation is less straightforward. For each 10% increase in *budget* we expect to see a `r m5b4`% increase in *gross revenue*. This term is both statistically and practically signficant. It shows that increasing the *budget* is not always profitable in-and-of-itself. The excess spending may only be worthwhile if it also increases the *movie quality* in a meaningful way. This is a question that could warrant future research.

> $\beta_5 = `r m5b5`$ : The final coefficient to evaluate is another interaction term. It serves to illustrate the difference in impact that increasing *budget* has on *PG-13* movies versus the *R* movie base case. This is another case where log transformations have been applied to both *gross revenue* and *budget*, making the coefficient less straightforward to interpret. For each 10% increase in *budget* we expect to see a `r m5b5`% increase in *gross revenue*. This term is both statistically and practically signficant. It suggests that increasing the *budget* of a *PG-13* movie has a greater positive effect on *gross revenue* than it does for an *R* movie. This helps support our hypothesis that the movie will perform better if we change from an *R* rating to a *PG-13* rating.


## Model Predictions

> Given the constraints of the movie as input parameters, model (5) was used to estimate the possible revenue of the Acme Studios superhero movie. The movie's budget is set at $50,000,000.00 and the MPAA rating can be either R or PG-13. Assuming that the IMDb score (or perceived quality) may vary, the estimated gross revenue predicted by the model also varies. The figure below shows the estimated gross revenue for different IMDb scores:

```{r model 5 Prediction, message=FALSE, warning=FALSE}

#model_5 = lm(log(adj_gross) ~ PG13 + score + PG13 * score + log(adj_budget) + log(adj_budget) * PG13, data = df)

BUDGET = 50000000

score_range = seq(0, 10, by = 0.001)
score_str = toString(round(mean(df$score),3))

df_predict_r = data.frame(
  score = score_range,
  PG13 = rep(factor(0), times = length(score_range)),
  adj_budget = rep(c(BUDGET), times = length(score_range))
)

df_predict_r$log_gross = predict(model_5, df_predict_r)
df_predict_r$gross = exp(df_predict_r$log_gross)

df_predict_pg13 = data.frame(
  score = score_range,
  PG13 = rep(factor(1), times = length(score_range)),
  adj_budget = rep(c(BUDGET), times = length(score_range))
)

df_predict_pg13$log_gross = predict(model_5, df_predict_pg13)
df_predict_pg13$gross = exp(df_predict_pg13$log_gross)

r_prediction = df_predict_r[df_predict_r$score == score_str, "gross"]
pg13_prediction = df_predict_pg13[df_predict_pg13$score == score_str, "gross"]
pg13_improve_est = pg13_prediction - r_prediction

gross_plot <- ggplot() +
      geom_point(data=df_predict_r, aes(x=score, y=gross, colour="darkblue")) + 
      geom_point(data=df_predict_pg13, aes(x=score, y=gross, colour="red")) + 
      ggtitle("Estimated Revenue vs Score\n") +
      xlab("Score") + ylab("Revenue (USD)") +
      theme(plot.title = element_text(hjust = 0.5)) +
      scale_color_hue(labels = c("R", "PG13")) +
      theme(legend.position = c(0.2, 0.85),
            legend.title = element_blank()) +
      annotate(geom="vline", xintercept = mean(df$score), y = (max(df_predict_pg13$gross)/2), linetype="dashed") +
      annotate(geom="text", x = mean(df$score), y = (max(df_predict_pg13$gross)), angle=90, vjust=-.5, hjust=1, label="Sample Mean IMDb Score") +
      annotate(geom="hline", yintercept = BUDGET, x = 0, linetype="dashed", color="darkgreen") +
      annotate(geom="text", x = 0, y = BUDGET,  vjust=-.5, hjust=0, label="Budget", color="darkgreen") 


gross_diff_plot <- ggplot() +
      geom_smooth(aes(x=df_predict_pg13$score, y=df_predict_pg13$gross - df_predict_r$gross), fill="darkgreen",
        colour="darkgreen", size=1) +
      ggtitle("Estimated Revenue Gain for\nchanging R film to PG-13") +
      xlab("Score") + ylab("PG-13 Revenue Gain (USD)") +
      theme(plot.title = element_text(hjust = 0.5)) +
      annotate(geom="vline", xintercept = mean(df$score), y = ((max(df_predict_pg13$gross)-max(df_predict_r$gross))/2), linetype="dashed") +
      annotate(geom="text", x = mean(df$score), y = (max(df_predict_pg13$gross)-max(df_predict_r$gross)), angle=90, vjust=-.5, hjust=1, label="Sample Mean IMDb Score")

ggarrange(gross_plot, gross_diff_plot,
                        nrow = 1, ncol = 2)
```

> According to the model, a PG-13 movie with this budget is expected to generate more revenue than an R movie. As the IMDb score (perceived quality) increases, the estimated gain in revenue increases at an exponential rate. If this model properly predicts the revenue, then it would suggest that there is much more to gain by allowing the movie to be rated PG-13. The horizontal dotted line on the left plot represents our movie's financial break-even point and shows that a PG-13 movie is expected to break-even at a lower IMDb score. Using the sample mean (`r score_str`) as an estimate for IMDb score (represented by the vertical dotted line on both plots), and holding all other variables constant, our model estimates the gross revenue for the movie to be $`r format(r_prediction,big.mark=",", trim=TRUE)` if rated R versus $`r format(pg13_prediction,big.mark=",", trim=TRUE)` if rated PG-13 (an improvement of $`r format(pg13_improve_est,big.mark=",", trim=TRUE)`). The predicted revenue for an R rating is dangerously close to our break-even point, whereas the gross revenue prediction for a PG-13 rating suggests a sizable profit margin.


# Limitations of your Model 

## Statistical limitations of your model

**Inflation**

> Initially, we were concerned that our data was not identically distributed because the movies range over 40 years. The cumulative price increase from 1980 to 2020 was 214%. This change undoubtedly affects our outcome variable, *gross revenue*. We used the consumer price index (CPI) to calculate an inflation adjustment for *gross revenue* and *budget*. While it is not a perfect adustment, accounting for inflation allows us to level the playing field for movies produced across the time span.

**Common Themes**

> Sequels and movies with common themes are not independent of each other. The success of one Star Wars movie directly impacts the success of another one. Specific movie themes and genres can also generate more attention at times. We did not take action to adjust for this. In future modeling, we can look at clustering by genre and movie theme.

**Year**

> Powerful movies often capture the attention of the nation and influence the types of films people view in a given year. In 2014, Interstellar set off a chain reaction for the attention drawn to movies related to space. On the flipside, if there's a series of bad Will Ferrell comedies produced, people might be less inclined to view some of those more explicit comedies. Clustering by decade or five-year time ranges could help reduce the impact of year on our model.


## Structural limitations of your model

> While our model accounts for many important variables of interest, other factors could impact our final model. We will discuss how omitted variables might affect each of our explanatory variables and what steps could resolve any omitted variable bias.

**Production Company**

> Different movie production companies have their niche within the Hollywood community. Certain companies will draw a strict line on violent, sexual, and other explicit content in their films. Those factors play a large role in *MPAA rating*. For example, Pixar Animation Studios, a subsidiary of Walt Disney  Studios, consistently produces high-quality movies with a *G* or *PG* rating. While initially aimed for an adolescent population, the movies have gained success among all age groups and generated *gross revenue* reaching 1 billion dollars. Well-known production companies with specific standards for content will likely still make movies with high *gross revenue*.
Additionally, movie critics might be partial to certain companies. Therefore, we expect the production company to bias the *MPAA rating* and *IMDb score* away from zero. We would need to find quality data on production companies to resolve this issue.

**Popular Actors**

> There are a lot of similarities between the argument for the production company and famous actors. Actors can have the same standards for content. Additionally, actors gain cult-like followings among the public. People will insist on watching all Matthew McConaughey or Julia Roberts movies regardless of the quality or content. Both known factors will influence the bias for *MPAA rating* and *IMDb score* away from zero. We would need to investigate actor popularity to resolve this bias.

**Movie Genre**

> The genre of the movie often affects the *MPAA rating*. An action movie will have more violence, and a comedy will likely have more explicit language. The genre will impact movie quality and viewership because people gravitate to certain genres. The bias impact is up for debate and not necessarily clear. While certain genres will drive up the *MPAA rating*, others will drive it down. We need more information on the popularity of genres to see their impact on movie scores.

# Conclusion

> For this research study, we expected to show that both *MPAA rating* and movie quality impact the film's *gross revenue*. In particular, we wanted to focus on how much more revenue a movie studio expects to make on a film that gets a *PG-13* rating instead of an *R* rating, holding other factors constant. We did this by creating a model that does the following: measures the effect of changing the *MPAA rating* on the *gross revenue* of the movie, the effect of a film's *IMDb score* on *gross revenue*, the interaction of *MPAA rating/budget* on *gross revenue*, and the effect of *budget* on *gross revenue*. The combination of all these effects proved to be the most effective based on the adjusted R-squared score and because of its best-quality predictions. Using an F-test reinforced our decision to reject the null hypothesis (such as in model 2), in favor of model 5.

> According to this model (5), we can see how a *PG-13* film with a budget of $0 and an *IMDb score* of 0 is expected to make 91.24% *less* *gross revenue* than a rated-*R* movie. However, when looking at the coefficient of the interaction term for *PG13* * *Budget*, we can see how $\beta_5$ is positive and statistically significant, which suggests that as the budget increases, the *PG-13* movies will generate more *gross revenue*. The negative coefficient might have mitigated this for *PG-13* * *IMDb score*, where, as the score increases, *PG-13* movies are expected to yield 4.51% less *gross revenue*. But it seems statistically insignificant because the p-value is not within the cut-off for *and* its 95% confidence interval (+/- 2 standard deviations) contains zero. 

> Based on these results, our model seems to favor *R* movies with a lower *budget* but favors *PG-13* movies with a higher *budget* with an R-Squared value of 0.428.

> As a result, our study seems conclusive about the causal relationship between *rating* and *gross revenue* if the *budget* is constant at $5,000,000. We do not fail to detect a significant causal effect between these variables because we see a trend in the increase in *gross revenue* for changing the rating from *R* to *PG-13*, as shown above in our model prediction graphs. The predictive model showed how the predicted revenue for an *R* rated movie does not suggest a sizable profit margin compared to the predicted *gross revenue* of *PG-13*. The model estimates an improvement of over $40,000,000 to change the rating from *R* to *PG-13*. Regarding our original problem statement, we would suggest modifying the movie for a more family-friendly rating.

> Given the limitations in our data, including the fact that it is not entirely IID and the subjective interpretation of what makes a movie "high" quality based on a good score, it is interesting to see our estimation models detect an effect for our variables of interest (*gross revenue*). Our theoretical causal model accurately captured the existing causal paths using F-tests and OLS regression under classical linear model assumptions to find an effect. For future refinement, perhaps we could work with a larger sample size (with movies before 1980) or further ensure that we have a truly random sample with equally distributed genres of movies and compare their models and R-squared values to see if there is a similar effect. 
