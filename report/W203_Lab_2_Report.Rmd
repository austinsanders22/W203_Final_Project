---
title: "Keeping it PG-13"
author: "Steve Hewitt, Martin Lim, Fidelia Nawar, & Austin Sanders"
date: "12/9/2021"
output: 
  pdf_document: 
    fig_caption: true
    number_sections: true
---

\tableofcontents
\newpage

```{r - Setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

MIN_REVENUE = 1
MIN_BUDGET = 1

library(gridExtra)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(hash)
library(lmtest)
library(sandwich)
library(stargazer)
library(hash)
library(caTools)
library(GGally)

# data <- read.csv(file = "../data/movies.csv")
# cpi <- read.csv(file = "../data/CPI_by_Year.csv")

data <- read.csv(file = "movies.csv")
cpi <- read.csv(file = "CPI_by_Year.csv")
```

# Introduction

## Context

> Acme Studios has spent a $50,000,000.00 budget on a superhero movie, and the director insists that the movie should include a scene where the main villain goes on an expletive-laden tirade. We know that including this scene will mean that the movie will be rated R, and cutting the scene will result in the movie being rated PG-13. The director is extremely upset that we want to cut the scene and says we're ruining the film's artistic integrity by trying to make editorial changes after the director's cut. So upset that he went directly to the studio head to complain. Now Acme Studiosâ€™ executive team has to decide: do they modify the movie for a more family-friendly rating, or do they respect the director's wishes and release it as-is? As data scientists, we would have difficulty quantifying artistic integrity or the value of the relationship between the studio and the director. Still, we feel strongly that we can show the relationship between worldwide revenue for a PG-13 vs. R ratings (holding all other variables constant). The studio head wants to know: How much more money do they expect to make by defying the director's wishes and cutting the movie to make it PG-13?

## Research Question

>   *Holding other factors constant, how much more money should a movie studio expect to make on a film that gets a PG-13 rating instead of an R rating from the MPAA?*

>   Our research question intends to measure the impact *MPAA Rating* and *Movie Quality* on the *Gross Revenue* that it will generate. Given other available, quantifiable factors like the *Budget*, *Genre*, and *Runtime*, this study also intends to investigate if they affect the *MPAA Rating* and/or *Movie Quality* which in-turn affect the *Gross Revenue*. 

## Causal Theory

![](causal_theory.png){width=60%}


>   Our research question seeks to measure the impact of the *MPAA Rating* (more specifically, PG-13 versus R) on the *Gross box office revenue*. A movie typically receives an R rating by the MPAA for some combination of violent content, adult content, explicit language, and drug use. In addition to these factors contributing to the MPAA rating, they also contribute to the quality of the movie. We expect to show that both MPAA rating and movie quality impact the gross revenue of the film. By adjusting the movie's content to secure the desired rating, we may also affect the quality of the movie. Therefore, this study will explore models that include a proxy for movie quality to attempt to minimize omitted variable bias.

# Research Design and Data

## Data Source

>   The data used for this study is a movie dataset from Kaggle (https://www.kaggle.com/danielgrijalvas/movies). It contains 7512 unique movie titles ranging from the year 1980 to 2020. According to the description of the creator of the dataset, the data was scraped from IMDb.com so the extent of the movie title coverage can go as far as the available information posted on the IMBd website. Below are the important columns that were considered for this study:

* Outcome Variable
    + `gross`   : Revenue of the movie in USD
\
* Explanatory Variables
    + `score`   : Average IMDb user rating
    + `rating`  : MPAA rating of the movie (R, PG, etc.)
    + `runtime` : Length of movie in minutes
    + `budget`  : Budget of the movie in USD
    + `votes`   : Number of user votes on the IMBd website
    
## Research Design

>   Using the data and variables above, this study measured the impact *MPAA Rating* and *Movie Quality* on the *Gross Revenue* that a movie will generate. Causal models were generated using the logic from the causal theory in 1c. `gross` is the main quantified success variable. According to the causal theory, two main explanatory quantities were included in the model. `rating` was used to quantify the MPAA rating of the film while `score` is the main quantified measurement of quality. `budget` and `runtime` both affect the the movie quality, but not the `rating` so they were considered as proxies for quality.

>   The outcome of interest is `gross`, the revenue of the movie in USD. This is the main outcome variable of all the causal models and is quantified in numeric form as is.

>   In order to properly quantify `rating` in the model, indicator variables were used. The data was divided into two (2) categories: PG-13, and R. All movies rated PG-13 were given a separate indicator variable (`PG13`) while all movies rated R were treated as the base-case.

>   Measuring movie quality is primarily measured by the `score` variable. Variables like `budget` and `runtime` also have an effect on quality so they were considered as possible proxies for quality in the causal models.

>   In order to remove duplicate movie titles, the data point with the larger budget was retained. If both the budgets and titles were equal, the data point with the larger revenue was retained.

>   Because our dataset covers a period of 50 years, we are applying a CPI-based price adjustment to each monetary variable to account for inflation. The inflation-adjusted valuse (in 2020 dollars) will be represented with the 'adj_gross' and 'adj_budget' variables.

>   The filtered dataset was used to produce multiple linear models and evaluate them using coefficient tests in R. Stargazer was used to compared the models to determine which model best aligns with the data and our causal theory. Armed with this chosen model, its predictions were evaluated to check how well the model is able to predict the revenue based on the input parameters. Finally, the model was applied to the specific case outlined in the overview section above to predict the revenue for both a PG-13 and R MPAA rating case.

## Data Cleaning
>   Removed entries with budget under `r MIN_BUDGET` or gross revenue under `r MIN_REVENUE` to filter out small-scale releases that do not fit the mold of the type of movie we want to measure.

>   Removed entries that were not rated PG-13 or R.

>   Removed duplicate entries as discussed in the previous section.

>   Created indicator variable for PG-13 rating.

>   Created CPI-adjusted variables for gross revenue and budget.


```{r Data Cleaning}
# Retrieve only the needed columns
df_raw = select(data, c('title', 'gross', 'budget', 'score', 'runtime', 'rating', 'year', 'votes'))

# Remove data points with world_revenue under MIN_REVENUE
df_raw <- subset(df_raw, df_raw$gross >= MIN_REVENUE & !is.na(df_raw$gross))

# Remove data points with budget under MIN_BUDGET
df_raw <- subset(df_raw, df_raw$budget >= MIN_BUDGET & !is.na(df_raw$budget))

# Keep PG-13 and R data points
df_raw <- subset(df_raw, df_raw$rating == "PG-13" | df_raw$rating == "R")

# Remove Duplicate Movie Titles
# Hash object for title : budget
h <- hash() 

# Clean dataframe
df = data.frame()

for(i in 1:nrow(df_raw)) {       # for-loop over rows
  title_key = df_raw[i,'title']
  
  if (TRUE == all(has.key( title_key, h ))) {
    # Title is already recorded

    # Search for existing row in clean dataframe with the same title
    for (k in 1:nrow(df)) {
      
      if (title_key == df[k,'title']) {
        # Replace row if the budget of the new value is higher than that of the
        # budget of the recorded title
        if (df_raw[i, 'budget'] > df[k, 'budget']) {
          
          # Delete found row in cleaned dataframe
          df = df[-c(k),]
          
          # Bind raw dataframe row to clean dataframe
          df <- rbind(df, df_raw[i,])
          
          # Revise title_key and budget to hash
          h[[title_key]] = df_raw[i,'budget']
        } else if (df_raw[i, 'budget'] > df[k, 'budget']
                   & df_raw[i, 'gross'] > df[k, 'gross']) {
          # Delete found row in cleaned dataframe
          df = df[-c(k),]
          
          # Bind raw dataframe row to clean dataframe
          df <- rbind(df, df_raw[i,])
          
          # Revise title_key and budget to hash
          h[[title_key]] = df_raw[i,'budget']
        }
        break
      }
      
    }
    
  } else {
    # Add title_key and budget to hash
    h[[title_key]] = df_raw[i,'budget']
    
    # Bind raw dataframe row to clean dataframe
    df <- rbind(df, df_raw[i,])
  }
}

# Apply Indicator variable for PG-13 (R is the base case)
df$PG13 = factor(ifelse(df$rating == "PG-13" , 1, 0))

# Perform CPI adjustment to both gross and budget variables.
df <- merge(x = df, y = cpi, by= 'year')
df <- transform(df, adj_gross = gross * adjust_to_2020_dollars, adj_budget = budget * adjust_to_2020_dollars)
```

## Exploratory Data Analysis - Fidelia

>   There are `r nrow(df)` unique titles considered for this study.

```{r - EDA: Check main financial variables}
gross_histogram <- df %>%
  ggplot(aes(adj_gross)) +
  geom_histogram(bins=30)

log_gross_histogram <- df %>%
  ggplot(aes(log(adj_gross))) +
  geom_histogram(bins=30)

budget_histogram <- df %>%
  ggplot(aes(adj_budget)) +
  geom_histogram(bins=30)

log_budget_histogram <- df %>%
  ggplot(aes(log(adj_budget))) +
  geom_histogram(bins=30)

grid.arrange(gross_histogram, log_gross_histogram,
             budget_histogram, log_budget_histogram,
             nrow = 2, ncol = 2, top = "Financial Variable distributions")

```

>   Analysis on each of the budget and gross variables.

```{r - EDA: Check other explanatory variables, warning=FALSE}
# CHECK OTHER EXPLANATORY VARIABLES

df$ratings_distribution = factor(ifelse(df$rating == "R", "R", ifelse(df$rating == "PG-13", "PG-13", "G/PG")))

ratings_distribution_histogram <- df %>%
  ggplot(aes(x=ratings_distribution)) +
  geom_bar() +
  theme(legend.position = "none")

score_histogram <- df %>%
  ggplot(aes(score)) +
  geom_histogram(bins=30)

year_histogram <- df %>%
  ggplot(aes(year)) +
  geom_histogram(bins=10)

votes_histogram <- df %>%
  ggplot(aes(votes)) +
  geom_histogram(bins=30)

log_votes_histogram <- df %>%
  ggplot(aes(log(votes))) +
  geom_histogram(bins=30)

runtime_histogram <- df %>%
  ggplot(aes(runtime)) +
  geom_histogram(bins=30)

grid.arrange(ratings_distribution_histogram, score_histogram,  
             votes_histogram, log_votes_histogram,
             runtime_histogram, year_histogram,
             nrow = 3, ncol = 2, top = "Explanatory Variable Distributions")
```

>   Analysis on each of the six graphs.

>   Based on our EDA, log transformations of both the gross revenue (`gross`) and `budget` variables seemed helpful to produce a better-fitting linear model. This is because of the skewed nature of the data where both the financial variables have most data points toward zero with some extreme outliers (movies that generated a very large revenue). Other numeric variables did not seem to require any transformations because the distribution seemed symmetrical enough.

```{r - Evaluate numeric variables for perfect collinearity, message = FALSE, warning = FALSE, error = FALSE}

df_collinearity = select(df, c('adj_gross', 'adj_budget', 'score', 'runtime', 'year', 'votes'))

# Log-Transform `adj_gross`, `adj_budget`, and `votes`
df_collinearity$log_adj_gross = log(df_collinearity$adj_gross)
df_collinearity$log_adj_budget = log(df_collinearity$adj_budget)
df_collinearity$log_votes = log(df_collinearity$votes)

# Remove `gross`, `budget`, and `votes`
df_collinearity = select(df_collinearity, c('log_adj_gross', 'log_adj_budget', 'score', 'runtime', 'year', 'log_votes'))

# check for collinearity
ggpairs(df_collinearity) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

>   According to the correlation plots above, the explanatory variables do not seem to have any strong correlation to each other. The log-transformed data for `adj_budget` and `votes` may have decent correlation with the log-transformed data for `adj_gross` (the outcome variable), but this should not produce any problems for finding an BLP for this data.


# Statistical Model - rearranged, move some parts to results

## A Model Building Process

```{r - Create the models}

#Model 1: Rating Only
# Base Case - R Movies
model_1 = lm(log(adj_gross) ~ PG13, data = df)

#Model 2: Rating and Quality
# Base Case - R Movies with rating of 0
model_2 = lm(log(adj_gross) ~ PG13 + score, data = df)

#Model 3: Rating and Quality with interaction terms
model_3 = lm(log(adj_gross) ~ PG13 + score + PG13*score, data = df)

#Model 4: Rating and Quality with budget
model_4 = lm(log(adj_gross) ~ PG13 + score + log(adj_budget), data = df)

#Model 5: Rating and Quality with budget and interaction terms
model_5 = lm(log(adj_gross) ~ PG13 + score + PG13 * score + log(adj_budget) + log(adj_budget) * PG13, data = df)

```

```{r - Function to interpret coefficients}
level_to_log <- function(coeff) {
  # Inputs the coefficient of an untransformed variable
  # Returns the % change in the log-transformed outcome variable per unit increase in coeff
  return ((exp(coeff)-1)*100)
}

log_log_pct_change <- function(coeff, pct_change) {
  # Inputs the coefficient of an untransformed variable and the percent change
  # Returns the % change in the log-transformed outcome variable given the % increase in the log-transformed explanatory variable
  # Basis: https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9
  input_ratio = 1+(pct_change/100)
  output_ratio = input_ratio ^ coeff
  return ((output_ratio-1)*100)
}

```

Our primary input variable of interest is the *MPAA rating*, with an *R* rating as the base case and a *PG-13* rating as the alternative case. *MPAA rating* is a categoric variable with most of our dataset falling under the *R* rating and a sizable minority falling under the *PG-13* rating.

Our causal theory has identified *Movie Quality* as another key input variable to measure because there is a relationship between the factos that determine *MPAA rating* and *Movie Quality*, and both also influence *gross revenue*. Because we don't have an exact measurement of *Movie Quality* we are using the IMDb Rating (*score*) as a proxy to help mitigate omitted variable bias. The distribution of this variable seems roughly normally distributed so no transformations were applied.

Lastly, we are also prepared to consider a model that includes *budget* as a parameter because it has a strong correlation to *gross revenue*, and including it has the potential to make the predictions from our model much more accurate even if it is not directly named in our causal theory. Including *budget* has the potential to absorb some of the effect we are measuring, but there is also the potential of value in examining the interaction between *budget* and *MPAA rating*; we may find that a certain rating provides better return as the budget increases, or conversely that a certain rating does well with even very small budgets. This variable has a lopsided distribution with a heavy tail near zero and quickly becomes sparse moving to the right. Applying a log transformation makes this variable appear more evenly distributed.

We took an iterative approach to model building by starting with something very simple and adding terms as we went along until we reached something that satisfied our need for something that was both significant and sufficiently predictive.

\


## Proposed Models:

\
(1) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 $$

We started off with the most simplistic possible model to see if a measurable difference between the *gross revenue* of an *R* or *PG-13* rated movie existed. This first model (1) measures only the effect of changing the MPAA rating (PG-13 or R) on the *gross revenue* of the movie. $\beta_0$ is the *gross revenue* that an *R*-rated movie is estimated to generate while $\beta_1$ is the estimated gain in *gross revenue* if the movie is instead rated *PG-13*. This model is too simple to be our final choice, but if it had shown no significant difference in *gross revenue* it would have been a major red flag.


\
(2) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 + \beta_2 * Score $$

The second model (2) introduced the effect of a film's *IMDb score* on the *gross revenue* on top of the components of model (1). $\beta_2$ indicates the estimated percentage increase in *gross revenue* per one point increase in *IMDb score*. Including *score* was considered a bare minimum requirement for our final model, because we are using it as a proxy for *Movie Quality* from our causal theory.


\
(3) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 + \beta_2 * Score + \beta_3 * PG13 * Score $$

The third model (3) introduced an interaction term between a film's *IMDb score* and *rating*, alongside the components of model (2). $\beta_3$ indicates the estimated percentage increase in revenue per one point increase in *IMDb score* if the movie is rated *PG-13*. Like in the previous model $\beta_2$ indicates the estimated percentage increase in *gross revenue* per one point increase in *IMDb score*, but having an interaction term allows *score* to impact predictions for *PG-13* and *R* differently. The magnitude of $\beta_3$ shows if the change in *gross revenue* per point of *score* is different for a *PG-13* movie than it is for an *R* movie.


\
(4) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 + \beta_2 * Score + \beta_3 * ln(Budget)$$

The fourth model (4) was built on top of model (2), with the addition of a term for *budget*. Unlike model (3), an interaction term between *MPAA rating* and *score* was not included. The purpose of this model was to offer something with more predictive power than the previous models by including *budget*. We know that *gross revenue* and *budget* are strongly correlated and we expected to see this model perform better at predicting results (as measured by adjusted R-squared) than the previous models. The term $\beta_3$ in this model represents the relationship between percentage increases in *budget* and percentage increases in *gross revenue*; for every 1% increase in *budget* we expect a $\beta_3$% increase in *gross revenue*.


\
(5) $$ ln(Gross) = \beta_0 + \beta_1 * PG13 + \beta_2 * Score + \beta_3 * PG13 * Score$$
$$+ \beta_4 * ln(Budget) + \beta_5 * PG13 * ln(Budget) $$

The fifth and final model (5) builds on all of the models before it. The terms $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ have identical meanings to model (3). $\beta_4$ now represents the relationship between percentage increases in *budget* and percentage increases in *gross revenue*. The new term $\beta_5$ is an interaction term between *MPAA rating* and *budget*. Having an interaction term allows *budget* to impact predictions for *PG-13* and *R* differently. The magnitude of $\beta_5$ shows if the percentage change in *gross revenue* per percentage increase in *budget* is different for a *PG-13* movie than it is for an *R* movie.


# Results

```{r create stargazer, results = "asis"}
stargazer(
  model_1, 
  model_2,
  model_3,
  model_4,
  model_5,
  type = 'latex',
  header = FALSE, 
  omit.stat=c("f", "ser"),
  star.cutoffs = c(0.05, 0.01, 0.001) # the default isn't in line with w203
)
```

Of all models considered, our team determined that model (5) was the most effective. It produces the best-quality predictions of all models, as shown by the adjusted R-squared score. It also captures the all of the effects that any of the models flagged as being significant. The one term that is not significant in this model is the interaction term between PG-13 and score, with a standard error that overlaps zero. A closer look at each of the model coefficients and how they can be interpreted:

```{r model 5 Beta 0 interpretation}
m5b0 = round(exp(summary(model_5)$coefficients[1]), digits = 2)
```

> $\beta_0$ - As a baseline, movies rated *R* with a *budget* of 0 and an *IMDb score* of 0 are expected to generate $`r format(m5b0, nsmall = 2)` in gross revenue.

```{r model 5 Beta 1 interpretation}
m5b1 = round(level_to_log(summary(model_5)$coefficients[2]), digits = 2)
```

> $\beta_1$ - The baseline for *PG-13* movies with a *budget* of 0 and an *IMDb score* of 0 is even lower, as they are expected to generate 
Movies rated PG-13 are likely to generate `r m5b1`% more gross revenue than a movie rated R.

```{r model 5 Beta 2 interpretation}
m5b2 = round(level_to_log(summary(model_5)$coefficients[3]), digits = 2)
```

> Movies are expected to generate `r m5b2`% more gross revenue per 1 point increase in score.

```{r model 5 Beta 3 interpretation}
m5b3 = round(log_log_pct_change(summary(model_5)$coefficients[4], 10), digits = 2)
```

> Movies are expected to generate `r m5b3`% more gross revenue if the budget is increased by 10%.

```{r model 5 Beta 4 interpretation}
m5b4 = round(level_to_log(summary(model_5)$coefficients[5]), digits = 2)
```

> Movies rated PG-13 are expected to generate `r m5b4`% more gross revenue than movies rated R per 1 point increase in score.

```{r model 5 Beta 5 interpretation}
m5b5 = round(log_log_pct_change(summary(model_5)$coefficients[6], 10), digits = 2)
```

> Movies rated PG-13 are expected to generate `r m5b5`% more gross revenue than movies rated R if the budget is increased by 10%.

# Limitations of your Model 

## Statistical limitations of your model

**Inflation**

We were initially concerned with our data not being identically distributed because the movies range over 40 years. The cumulative price increase from 1980 to 2020 was 214%. This change undoubtedly affects our outcome variable, gross revenue. We used the consumer price index (CPI) to calculate an inflation adjustment for gross revenue and the budget. Accounting for inflation allows us to level the playing field for movies produced across the time span.

**Common Themes**

Sequels and movies with common themes are not independent of each other. The success of one Star Wars movie directly impacts the success of another one. Specific movie themes and genres can also generate more attention at times. We did not take action to adjust for this. In future modeling, we can look at clustering by genre, year, and movie theme.

**Best Linear Predictor**

We ensured that there were not any concerns with co-linearity and non-finite variance within our data modeling. Do we need to elaborate on this? According to guidelines, we only need to highlight assumptions that pose problems for analysis.


## Structural limitations of your model

While our model accounts for many important variables of interest, there are other factors that could have an impact on our final model. We will discuss how omitted variables might affect each of our explanatory variables and what steps could resolve any omitted variable bias.

**Production Company**

Different movie production companies have their niche within the Hollywood community. Certain companies will draw a strict line on violent, sexual, and other explicit content in their films. Those factors play a large role in MPAA rating. For example, Pixar Animation Studios, a subsidiary of Walt Disney  Studios, consistently produces high quality movies with a G or PG rating. While initially aimed for an adolescent population, the movies have gained success among all age groups and generated revenue reaching 1 billion dollars. Well known production companies with specific standards for content will likely still make movies with a high gross revenue. Additionally, movie critics might be partial to certain companies. Therefore, we expect the production company to bias the MPAA rating and quality score away from zero. We would need to find quality data on production companies to resolve this issue.

**Popular Actors**

There are a lot of similarities between the argument for production company and for popular actors. Actors can have the same standards for content. Additionally, actors gain cult-like followings among the public. People will insist on watching all Matthew McConaughey or Julia Roberts movies regardless of the quality or content. Both known factors will influence the bias for MPAA rating and quality score away from zero. We would need to investigate actor popularity to resolve thsi bias.

**Movie Genre**

Genre of movie often affects the MPAA rating. An action movie will have more violence, and a comedy will likely have more explicit language. The genre will impact movie quality and viewership because people gravitate to certain genres. The bias impact is up for debate and not necessarily clear. While certain genres will drive up the MPAA rating, others will drive it down. We need more information on popularity of genres to see its impact on movie scores.

# Conclusion - Fidelia

Make sure that you end your report with a discussion that distills key insights from your estimates and addresses your research question.



